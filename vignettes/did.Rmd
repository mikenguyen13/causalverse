---
title: "2. Difference-in-Differences"
author: "Mike Nguyen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{2. Difference-in-Differences}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(causalverse)
library(tidyverse)
```

# Assumptions

## Treatment Variation Plot

```{r}
library(PanelMatch)
library(fixest)

DisplayTreatment(
    unit.id = "id",
    time.id = "year",
    # legend.position = "none",
    xlab = "Year",
    ylab = "Unit",
    # hide.x.tick.label = TRUE, 
    hide.y.tick.label = TRUE,
    # dense.plot = TRUE,
    treatment = "treat",
    data = fixest::base_stagg |> 
      mutate(treat = if_else(time_to_treatment < 0, 0, 1))
)
```
It's okay to have some units without any observation on the left-hand side (i.e., left-censored).

## Pre-treatment Parallel Trends

### plot_par_trends

The `plot_par_trends` function is designed to assist researchers and analysts in visualizing parallel trends in longitudinal datasets, particularly for datasets with treatment and control groups. This tool makes it easy to visualize changes over time for various outcome metrics between the groups.

**Data Structure**

For optimal use of `plot_par_trends`, ensure your data is structured in the following manner:

-   `entity`: A unique identifier for each observation (e.g., individuals, companies).
-   `time`: The time period for the observation.
-   Treatment status column: Distinguishing treated observations from controls.
-   Metric columns: Capturing the outcome measures of interest.

**Sample Data Generation**

For demonstration purposes, we can generate some illustrative data:

```{r}
library(tidyverse)
data <- expand.grid(entity = 1:100, time = 1:10) %>%
  dplyr::arrange(entity, time) %>%
  dplyr::mutate(
    treatment = ifelse(entity <= 50, "Treated", "Control"),
    outcome1 = 0.5 * time + rnorm(n(), 0, 2) + ifelse(treatment == "Treated", 0, 0),
    outcome2 = 3 + 0.3 * time + rnorm(n(), 0, 1) + ifelse(treatment == "Treated", 0, 2),
    outcome3 = 3 + 0.5 * time * rnorm(n(), 0, 1) + rexp(n(), rate = 1) + ifelse(treatment == "Treated", 0, 2), 
    outcome4 = time + rnorm(n(), 0, 1) + ifelse(treatment == "Treated", 0, 2) * 2
  )
```

Visualizing Trends with `plot_par_trends` Invoke the `plot_par_trends` function using the sample data:

```{r}
results <- plot_par_trends(
  data = data,
  metrics_and_names = list(outcome1 = "Outcome 1", outcome2 = "Outcome 2", outcome3 = "Outcome 3", outcome4 = "Outcome 4"),
  treatment_status_var = "treatment",
  time_var = list(time = "Time"),
  smoothing_method = "glm",
  theme_use = causalverse::ama_theme(base_size = 12)
  # title_prefix = "Para Trends"
)
```

**Note**: This custom function is built based on the `geom_smooth` function from ggplot2. Therefore, it supports most of the smoothing methods you'd find in ggplot2, such as `lm`, `glm`, `loess`, etc.

The function returns a list of ggplot objects, which can be visualized using tools like `gridExtra`

```{r}
library(gridExtra)
gridExtra::grid.arrange(grobs = results, ncol = 2)
```

**Note of Caution**: When using this custom package, it's crucial to carefully examine parallel trends plots both with and without control variables. At times, one might observe suitable parallel trends without control variables. However, when these control variables are introduced, the underlying assumptions can be disturbed. Conversely, there are cases where the general parallel trends assumption doesn't seem to be in place, but when conditioned on the control variables, the trends align perfectly. This package provides functions that allow users to easily generate these plots side by side, especially after formulating the predicted values of dependent variables by accounting for control variables. Always approach these plots with a discerning eye to ensure accurate interpretation and application.

In this demonstration, I've incorporated the use of the smoothing function to illustrate its potential application. It's imperative, however, to approach this function with prudence. Although the smoothing function can be invaluable in revealing underlying trends in specific scenarios, its application carries inherent risks. One such risk is the inadvertent or intentional misrepresentation of data, leading observers to falsely deduce the presence of parallel trends where they might not exist. This can potentially skew interpretations and lead to incorrect conclusions.

Given these concerns, if you're inclined to utilize the smoothing function, it's paramount to also consider the implications and insights from our secondary plot. This subsequent visualization offers a more granular and statistically sound perspective, specifically focusing on the pre-treatment disparities between the treated and control groups. It serves as a vital counterbalance, ensuring that any trend interpretations are grounded in robust statistical analysis.

### plot_coef_par_trends

Arguments

- `data`: A data frame containing the data to be used in the model.
- `dependent_vars`: A named list of dependent variables to model along with their respective labels.
- `time_var`: The name of the time variable in the data.
- (similarly, describe other arguments...)

Output

The function returns a plot or a list of plots visualizing interaction coefficients based on user specifications.


```{r}
library(fixest)
data("base_did")

# Combined Plot
combined_plot <- plot_coef_par_trends(
  data = base_did,
  dependent_vars = c(y = "Outcome 1", x1 = "Outcome 2"),
  time_var = "period",
  unit_treatment_status = "treat",
  unit_id_var = "id",
  plot_type = "coefplot",
  combined_plot = TRUE,
  plot_args = list(main = "Interaction coefficients Plot"),
  legend_title = "Metrics",
  legend_position = "bottomright"
)

# Individual Plots
indi_plots <- plot_coef_par_trends(
  data = base_did,
  dependent_vars = c(y = "Outcome 1", x1 = "Outcome 2"),
  time_var = "period",
  unit_treatment_status = "treat",
  unit_id_var = "id",
  plot_type = "coefplot",
  combined_plot = FALSE
)

```



# Random Treatment Assignments

In Difference-in-Differences analysis, ensuring randomness in treatment assignment is crucial. This randomization comes in two main levels: random time assignment and random unit assignment.

1.  [Random Time Assignment]

    -   **Definition:** This pertains to when (in time) a treatment or intervention is introduced, not to which units it's introduced.

    -   **Importance in Staggered DiD or Rollout Design:** If certain periods are predisposed to receiving treatment (e.g., economic booms or downturns), then the estimated treatment effect can get confounded with these period-specific shocks. A truly random time assignment ensures that the treatment's introduction isn't systematically related to other time-specific factors.

    -   **Example:** Suppose there's a policy aimed at improving infrastructure. If this policy tends to get introduced during economic booms because that's when governments have surplus funds, then it's challenging to disentangle the effects of the booming economy from the effects of the infrastructure policy. A random time assignment would ensure the policy's introduction isn't tied to the state of the economy.

2.  [Random Unit Assignment]

    -   **Definition:** This pertains to which units (like individuals, firms, or regions) are chosen to receive the treatment.

    -   **Example:** Using the same infrastructure policy, if it is always introduced in wealthier regions first, then the effects of regional affluence can get confounded with the policy effects. Random unit assignment ensures the policy isn't systematically introduced to certain kinds of regions.

**Providing Evidence of Random Treatment Assignments:**

To validate the DiD design, evidence should be provided for both random time and random unit assignments:

-   **For [Random Time Assignment]:**

    -   **Graphical Analysis:** Plot the timing of treatment introduction across various periods. A discernible pattern (like always introducing a policy before election years) can raise concerns.

    -   **Narrative Evidence:** Historical context might indicate that the timing of treatment introduction was exogenous. For example, if a policy's rollout timing was determined by some external random event, that would support random time assignment.

-   **For [Random Unit Assignment]:**

    -   **Statistical Tests:** Conduct tests to demonstrate that pre-treatment characteristics are balanced between the treated and control groups. Techniques like t-tests or regressions can be used for this.

    -   **Narrative Evidence:** Institutional or historical data might show that the selection of specific units for treatment was random.

## Random Time Assignment

```{r}
library(ggplot2)
library(gridExtra)
library(dplyr)

# Control number of units and time periods here
num_units   <- 2000
num_periods <- 20  

# Setting seed for reproducibility
set.seed(123)

# Generate data for given number of units over specified periods
data <- expand.grid(unit = 1:num_units, time_period = 1:num_periods)
data$treatment_random     <- 0
data$treatment_systematic <- 0

# Randomly assigning treatment times to units
random_treatment_times <- sample(1:num_periods, num_units, replace = TRUE)
for (i in 1:num_units) {
  data$treatment_random[data$unit == i & data$time_period == random_treatment_times[i]] <- 1
}

# Calculate peaks robustly
peak_periods <- round(c(0.25, 0.5, 0.75) * num_periods)

# Systematic treatment assignment with higher probability at peak periods
prob_values <- rep(1/num_periods, num_periods)

# Update the probability for peak periods; the rest will have a slightly reduced probability
higher_prob <- 0.10  # Arbitrary, adjust as necessary
prob_values[peak_periods] <- higher_prob
adjustment <- (length(peak_periods) * higher_prob - length(peak_periods) / num_periods) / (num_periods - length(peak_periods))
prob_values[-peak_periods] <- 1/num_periods - adjustment

systematic_treatment_times <- sample(1:num_periods, num_units, replace = TRUE, prob = prob_values)

for (i in 1:num_units) {
  data$treatment_systematic[data$unit == i & data$time_period == systematic_treatment_times[i]] <- 1
}
```

```{r}
# Plotting
plot_random <-
  plot_treat_time(
    data = data,
    time_var = time_period,
    unit_treat = treatment_random,
    theme_use = causalverse::ama_theme(base_size = 12)
  )

plot_systematic <-
  plot_treat_time(
    data = data,
    time_var = time_period,
    unit_treat = treatment_systematic,
    theme_use = causalverse::ama_theme(base_size = 12),
    show_legend = TRUE
  )

gridExtra::grid.arrange(plot_random, plot_systematic, ncol = 2)
```

**Random Time Assignment Plot:**

-   Treatment is dispersed randomly across time periods, with no clear pattern.

**Systematic Time Assignment Plot:**

-   Distinct peaks are observed at the 25th, 50th, and 75th periods, indicating non-random treatment assignment at these times.

**Interpretation:** The random plot shows arbitrary treatment assignments, while the systematic plot reveals consistent periods where more units receive treatment. This systematic behavior can introduce bias in causal studies.

## Random Unit Assignment

1.  **For a Single Pre-treatment Characteristic:**

    -   **T-test:** This is used for comparing means of the characteristic between two groups (treated vs. control). If the p-value is significant (typically \< 0.05), it suggests the groups differ on that characteristic.

```{r}
# data <- data.frame(treatment = c(rep(0, 50), rep(1, 50)),
#                    # Dummy for treatment
#                    characteristic = rnorm(100) # Randomly generated characteristic
#                    )

set.seed(123)
data = mtcars |> 
  dplyr::select(mpg, cyl) |> 
  dplyr::rowwise() |> 
  dplyr::mutate(treatment = sample(c(0,1), 1, replace = T)) |> 
  dplyr::ungroup()
                   
t.test(mpg ~ treatment, data = data)
```

-   **Regression:** Run a regression of the pre-treatment characteristic on the treatment dummy. If the coefficient on the treatment dummy is significant, it suggests the groups differ on that characteristic. The regression can be specified as:

$$
Characteristic_i = \alpha + \beta \times Treatment_i + \epsilon_i
$$

where $Characteristic_i$ is the pre-treatment characteristic of unit $i$ and $Treatment_i$ is a dummy variable which equals 1 if $unit_i$ is treated and 0 otherwise.

```{r}
lm_result <- lm(mpg ~ treatment, data = data)
summary(lm_result)
```

2.  **For Multiple Pre-treatment Characteristics:**

**Visualization**

-   For both single and multiple characteristics, it can be useful to visualize the distributions in both groups. Histograms, kernel density plots, or box plots can be employed to visually assess balance.
-   It's important to note that you should consider the pre-treatment periods.

```{r}
library(ggplot2)
library(rlang) 
library(gridExtra)

# Density distribution for a single characteristic
ggplot(data, aes(x = mpg, fill = factor(treatment))) +
  geom_density(alpha = 0.5) +
  labs(fill = "Treatment") +
  ggtitle("Density Distribution by Treatment Group") + 
  causalverse::ama_theme()

# Side-by-side density distributions for multiple characteristics
plot_list <- plot_density_by_treatment(
  data = data,
  var_map = list("mpg" = "Var 1",
                 "cyl" = "Var 2"),
  treatment_var = c("treatment" = "Treatment Name\nin Legend")
)

grid.arrange(grobs = plot_list, ncol = 1)
```

**Multivariate Regression**

Run a regression of each pre-treatment characteristic on the treatment dummy. This allows for the simultaneous assessment of balance on multiple characteristics. You can include all characteristics in a single regression as dependent variables.

```{r}
# data$characteristic2 <- rnorm(100)
variables <- data |> 
  dplyr::select(mpg, cyl)

lm_multivariate <-
  lm(cbind(mpg, cyl) ~ treatment, data = data)
summary(lm_multivariate)
```

All of the coefficients in each regression are not significant. Hence, we don't have any concerns.

To be more rigorous, we should estimate all regressions simultaneously using SUR if we suspect the error terms of the different regression equations are correlated.

```{r, message=FALSE}
# install.packages("systemfit")
library(systemfit)

equation1 <- mpg ~ treatment
equation2 <- cyl ~ treatment

system <-
  list(characteristic = equation1, characteristic2 = equation2)
fit <- systemfit(system, data = data, method = "SUR")
summary(fit)
```

For `mpg` and `cyl`: The treatment effect is not statistically significant, implying no evidence against random unit assignment based on this characteristic.

**Hotelling's T-squared test**

This is the multivariate counterpart of the T-test designed to test the mean vector of two groups. It's useful when you have multiple pre-treatment characteristics and you want to test if their mean vectors differ between the treated and control groups.

```{r}
# For Hotelling's T^2, you can use the `Hotelling` package.
# install.packages("Hotelling")
library(Hotelling)

treated_data <-
  data[data$treatment == 1, c("mpg", "cyl" )]
control_data <-
  data[data$treatment == 0, c("mpg", "cyl" )]

hotelling_test_res <- hotelling.test(treated_data, control_data)
hotelling_test_res
```

**Matching**

This is a more advanced technique, for example, modeling the probability of being treated based on pre-treatment characteristics using a logistic regression. After matching, you can test for balance in the pre-treatment characteristics between the treated and control groups.

```{r}
# For propensity score matching, use the `MatchIt` package.
# install.packages("MatchIt")
library(MatchIt)

m.out <-
  matchit(treatment ~ mpg + cyl,
          data = data,
          method = "nearest")
matched_data <- match.data(m.out)

# After matching, you can test for balance using t-tests or regressions.
t.test(mpg ~ treatment, data = matched_data)
```

In conclusion, while visualizations provide an intuitive understanding, statistical tests provide a more rigorous method for assessing balance.
